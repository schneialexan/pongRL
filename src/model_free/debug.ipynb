{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import tqdm\n",
    "from IPython.display import Image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongEnv(gym.Env):\n",
    "    def __init__(self, name, render_mode='rgb_array') -> None:\n",
    "        super(PongEnv, self).__init__()\n",
    "        self.env = gym.make(name, render_mode=render_mode)\n",
    "        self.total_timesteps = 0\n",
    "        self.memory = []\n",
    "        self.action_space = self.env.action_space\n",
    "    \n",
    "    def take_step(self, agent, action):\n",
    "        self.total_timesteps += 1\n",
    "            \n",
    "        next_frame, next_frames_reward, next_frame_terminal, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # return next_state, reward, done, _ \n",
    "        return next_frame, next_frames_reward, next_frame_terminal, truncated\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.env.reset()\n",
    "    \n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        return self.env._get_obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo(agent, episode):\n",
    "    G=0\n",
    "    visited_state_actions = set()  # To keep track of state-action pairs we've seen\n",
    "\n",
    "    for t in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[t]\n",
    "        state_str = str(state)\n",
    "        G = agent.gamma * G + reward\n",
    "\n",
    "        # Check if the state-action pair is visited for the first time in this episode\n",
    "        if not (state_str, action) in visited_state_actions:\n",
    "            visited_state_actions.add((state_str, action))  # Mark this state-action as visited\n",
    "            agent.returns[(state_str, action)].append(G)\n",
    "            agent.Q[state_str][action] = np.mean(agent.returns[(state_str, action)])\n",
    "    return agent\n",
    "\n",
    "def Q_Algorithm(agent, episode):\n",
    "    G=0\n",
    "    score = 0\n",
    "    for state, action, reward in reversed(episode):\n",
    "        state_str = str(state)\n",
    "        score += reward\n",
    "        G = agent.gamma * G + reward  # Calculate the cumulative reward\n",
    "        # Q-learning update\n",
    "        agent.Q[state_str][action] += agent.alpha * (G - agent.Q[state_str][action])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=1., epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.env = env\n",
    "        self.gamma = gamma  # Discount factor for future rewards\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.epsilon = epsilon  # Epsilon-greedy exploration parameter\n",
    "        self.epsilon_min = epsilon_min  # Minimum epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore by selecting a random action\n",
    "        else:\n",
    "            state_str = str(state)\n",
    "            return np.argmax(self.Q[state_str])  # Exploit by selecting the best action\n",
    "\n",
    "    def generate_episode(self):\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.policy(state)\n",
    "            next_state, reward, done, _ = self.env.take_step(self, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        return episode\n",
    "\n",
    "    def update(self, algorithm='Q-Learning'):\n",
    "        episode = self.generate_episode()\n",
    "        \n",
    "        if algorithm == 'Q-Learning':\n",
    "            self = Q_Algorithm(self, episode)\n",
    "        elif algorithm == 'MonteCarlo':\n",
    "            self = MonteCarlo(self, episode)\n",
    "        else:\n",
    "            raise Exception('Algorithm not implemented')\n",
    "        # Decay ε after each episode\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "    def train(self, num_episodes, algorithm='Q-Learning'):\n",
    "        for ep in tqdm.tqdm(range(num_episodes), desc='Training', unit='ep'):\n",
    "            self.update(algorithm=algorithm)\n",
    "        return self.Q\n",
    "            \n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        state_str = str(state)\n",
    "        return np.argmax(self.Q[state_str])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20000 [00:00<?, ?ep/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20000/20000 [3:39:20<00:00,  1.52ep/s]  \n"
     ]
    }
   ],
   "source": [
    "env = PongEnv('ALE/Pong-v5', render_mode='rgb_array')\n",
    "agt = PongAgent(env)\n",
    "theAgentQ = agt.train(20000, algorithm='MonteCarlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_episode(self, output_file):\n",
    "    episode = []\n",
    "    state = self.env.reset()\n",
    "    done = False\n",
    "    actions=[]\n",
    "    frames = []  # To store frames for the GIF\n",
    "    while not done:\n",
    "        action = self.policy(state)\n",
    "        actions.append(action)\n",
    "        next_state, reward, done, _ = self.env.take_step(self, action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "        # Render from state\n",
    "        frame = self.env.env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Save frames as a GIF\n",
    "    imageio.mimsave(output_file, frames, duration=1 / 60)  # Assuming 30 FPS\n",
    "    print('Saved the GIF: {}'.format(output_file))\n",
    "    print(f'Total Steps: {len(actions)}')\n",
    "    for i in range(3):\n",
    "        print(f'Action {i}: {actions.count(i)} = {actions.count(i)/len(actions)}%')\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\miniconda3\\envs\\dragonfly\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the GIF: pong_episode.gif\n",
      "Total Steps: 764\n",
      "Action 0: 0 = 0.0%\n",
      "Action 1: 1 = 0.0013089005235602095%\n",
      "Action 2: 762 = 0.9973821989528796%\n"
     ]
    }
   ],
   "source": [
    "episode = render_episode(agt, 'pong_episode.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.PongAgent.__init__.<locals>.<lambda>()>,\n",
       "            {'[[[  0   0   0]\\n  [  0   0   0]\\n  [  0   0   0]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n [[144  72  17]\\n  [144  72  17]\\n  [144  72  17]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n [[144  72  17]\\n  [144  72  17]\\n  [144  72  17]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n ...\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]]': array([-0.43206023, -0.43206888, -0.43205258, -0.43206178, -0.43207946,\n",
       "                    -0.43205683]),\n",
       "             'None': array([-0.00130111, -0.00129296, -0.00130422, -0.00117634, -0.00126609,\n",
       "                    -0.00104134])})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agt.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.PongAgent.__init__.<locals>.<lambda>()>,\n",
       "            {'[[[  0   0   0]\\n  [  0   0   0]\\n  [  0   0   0]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n [[144  72  17]\\n  [144  72  17]\\n  [144  72  17]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n [[144  72  17]\\n  [144  72  17]\\n  [144  72  17]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n ...\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]]': array([-0.25730191, -0.23752578, -0.00953974, -0.25206889, -0.28119456,\n",
       "                    -0.2589139 ]),\n",
       "             'None': array([-0.00134365, -0.00134365, -0.00134365, -0.00134365, -0.00134365,\n",
       "                    -0.00134365])})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defaultdict(<function __main__.PongAgent.__init__.<locals>.<lambda>()>,\n",
    "            {'[[[  0   0   0]\\n  [  0   0   0]\\n  [  0   0   0]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n [[144  72  17]\\n  [144  72  17]\\n  [144  72  17]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n [[144  72  17]\\n  [144  72  17]\\n  [144  72  17]\\n  ...\\n  [144  72  17]\\n  [144  72  17]\\n  [144  72  17]]\\n\\n ...\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]\\n\\n [[236 236 236]\\n  [236 236 236]\\n  [236 236 236]\\n  ...\\n  [236 236 236]\\n  [236 236 236]\\n  [236 236 236]]]': array([-0.25730191, -0.23752578, -0.00953974, -0.25206889, -0.28119456,\n",
    "                    -0.2589139 ]),\n",
    "             'None': array([-0.00134365, -0.00134365, -0.00134365, -0.00134365, -0.00134365,\n",
    "                    -0.00134365])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Q-learning for a Pong environment is possible, but it might not be the most effective approach. Q-learning is a model-free reinforcement learning algorithm that can work well for simple environments with a relatively small state and action space. Pong, on the other hand, is a more complex environment with a large state space due to the continuous nature of the screen pixels.\n",
    "\n",
    "In Q-learning, you maintain a Q-table that stores the expected cumulative rewards for each state-action pair. In the case of Pong, the state space would be huge because it would need to include all the possible screen configurations, which makes Q-learning impractical in this scenario. This table would be too large to store and update efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragonfly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
