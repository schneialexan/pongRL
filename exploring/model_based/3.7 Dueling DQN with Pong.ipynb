{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Q Network with Pong\n",
    "\n",
    "DQN was created by DeepMind researchers, achieving superhuman performance on many Atari games (https://deepmind.com/research/dqn/). What made the accomplishments of DQN even more impressive is that the DQN had nearly the same architecture and hyperparameter settings for each game (ie no game specific knowledge or set up) and the only inputs to the DQN were the screen pixels. The main parts of the DQN are Q learning, the neural net, experience replay, target network updating, and data/environment pre-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check and use GPU if available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer\n",
    "\n",
    "Q learning is an off-policy method. In off-policy methods we often save samples and then train on them. DQN has a memory buffer that stores state, action, reward, done condition, and next state samples. In the training loop, DQN will sample from this memory buffer and train on the saved samples. The replay buffer is essentially a queue that we randomly sample from. When the queue gets full we replace the first element of the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replay buffer from and some code based on https://github.com/sfujim/TD3\n",
    "\n",
    "# create replay buffer of tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "\n",
    "        return np.array(x), np.array(y), np.array(u).reshape(-1,1), np.array(r).reshape(-1,1), np.array(d).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Wrappers\n",
    "\n",
    "Wrappers modify gym environments. Wrappers can change the observation, reward, done, and info returns. Wrappers can also change the gym functions like step() and reset(). We include and use some common wrappers used in Atari games in the cells below. To use a wrapper, feed the gym env into the wrapper function like in the next cell. Sometimes problems require their own customized wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#env = gym.make('PongNoFrameskip-v4')\n",
    "#env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False) #scale in DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from: https://github.com/openai/baselines/baselines/common/atari_wrappers.py\n",
    "# from: https://github.com/Officium/RL-Experiments/blob/master/src/common/wrappers.py \n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if 0 < lives < self.lives:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few\n",
    "            # frames so it's important to keep lives > 0, so that we only reset\n",
    "            # once the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        shape = (2, ) + env.observation_space.shape\n",
    "        self._obs_buffer = np.zeros(shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = info = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    @staticmethod\n",
    "    def reward(reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grayscale = grayscale\n",
    "        shape = (1 if self.grayscale else 3, self.height, self.width)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=shape, dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, frame):\n",
    "        if self.grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        size = (self.width, self.height)\n",
    "        frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
    "        if self.grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "        return frame.transpose((2, 0, 1))\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also `LazyFrames`\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        shape = (shp[0] * k, ) + shp[1:]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=shape, dtype=env.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return np.asarray(self._get_ob())\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return np.asarray(self._get_ob()), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are\n",
    "        only stored once. It exists purely to optimize memory usage which can be\n",
    "        huge for DQN's 1M frames replay buffers.\n",
    "        This object should only be converted to numpy array before being passed\n",
    "        to the model. You'd not believe how complex the previous solution was.\n",
    "        \"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=-3)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "    \n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "    \n",
    "    \n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env, width=84, height=84)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pong\n",
    "\n",
    "In this notebook we'll train an agent to play the classic Atari game Pong. In Pong the agent controls a paddle and tries to hit a ball past the opponents paddle to score a point. The agent gets a positive reward when scoring a point and a negative reward when the opponent scores a point. The episode ends when a set number of points are reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schneialexan@edu.local/miniconda3/envs/rl/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:290: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n"
     ]
    }
   ],
   "source": [
    "#create Pong env and test it a bit\n",
    "env = gym.make('PongNoFrameskip-v4', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "for i in range(3000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, truncate, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network\n",
    "\n",
    "The Deep Q Network is from the original DQN paper and is sometimes referred to as the Nature CNN (since the article was published in Nature). Compared to state of the art CNNs in image recognition, the Nature CNN is quite simple. There are only three convolutional layers, followed by a fully connected (dense) layer and a linear layer. All but the linear layer have ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Deep Q Network Class by inheriting from torch.nn.Module\n",
    "# based on Nature CNN from OpenAI baselines: https://github.com/openai/baselines/blob/1b092434fc51efcb25d6650e287f07634ada1e08/baselines/common/models.py\n",
    "           \n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, action_size, hidden_size):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.conv_layer_1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv_layer_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc_layer = nn.Linear(7 * 7 * 64, hidden_size)\n",
    "        # V(s) value of the state\n",
    "        self.dueling_value = nn.Linear(hidden_size, 1)\n",
    "        # Q(s,a) Q values of the state-action combination\n",
    "        self.dueling_action = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_layer_1(x))\n",
    "        x = F.relu(self.conv_layer_2(x))\n",
    "        x = F.relu(self.conv_layer_3(x))\n",
    "        x = F.relu(self.fc_layer(x.view(x.size(0), -1)))\n",
    "        # get advantage by subtracting dueling action mean from dueling action\n",
    "            # then add estimated state value\n",
    "        x = self.dueling_action(x) - self.dueling_action(x).mean(dim=1, keepdim=True) + self.dueling_value(x)\n",
    "        return x\n",
    "\n",
    "# Non-dueling network\n",
    "\n",
    "# class DeepQNetwork(nn.Module):\n",
    "#     def __init__(self, action_size, hidden_size):\n",
    "#         super(DeepQNetwork, self).__init__()\n",
    "#         self.conv_layer_1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "#         self.conv_layer_2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "#         self.conv_layer_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "#         self.dense_layer = nn.Linear(7 * 7 * 64, hidden_size)\n",
    "#         self.out_layer = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x / 255. # image data is stored as ints in 0 to 255 range. Divide to scale to 0 to 1 range\n",
    "#         x = F.relu(self.conv_layer_1(x))\n",
    "#         x = F.relu(self.conv_layer_2(x))\n",
    "#         x = F.relu(self.conv_layer_3(x))\n",
    "#         x = F.relu(self.dense_layer(x.view(x.size(0), -1)))\n",
    "#         return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQNAgent Class\n",
    "\n",
    "We create a DQNAgent class. The agent has a train network for learning a policy and a target network for performing target network updates. The agent has a select_action() function for sampling an action based on the epsilon-greedy method. The agent has a train() function, in which the agent samples from the replay buffer and updates the neural network parameters to improve its policy. The agent also has a update_target_network() function for updating the target network parameters with the training network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, action_size, hidden_size, learning_rate ):\n",
    "        self.action_size = action_size\n",
    "        self.train_net = DeepQNetwork(action_size, hidden_size).to(device)\n",
    "        self.target_net = DeepQNetwork(action_size, hidden_size).to(device)\n",
    "        self.target_net.load_state_dict(self.train_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.train_net.parameters(), lr=learning_rate)\n",
    "\n",
    "        \n",
    "    def select_action(self, s, eps, ts):\n",
    "        # select action according to epsilon-greedy method\n",
    "        if np.random.rand() <= eps:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            print(ts, eps)\n",
    "            # greedy action is the largest Q value from the train network based on the input\n",
    "            with torch.no_grad():\n",
    "                input_state = torch.FloatTensor(np.array(s)).unsqueeze(0).to(device)\n",
    "                print(input_state.shape)\n",
    "                a = self.train_net(input_state).max(1)[1]#.view(1, 1)#.detach().cpu().numpy()[0]\n",
    "                a = int(a)\n",
    "        return a\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size, discount):\n",
    "        # train the training network\n",
    "        # sample a batch from the replay buffer\n",
    "        x0, x1, a, r, d = replay_buffer.sample(batch_size)\n",
    "        # turn batches into tensors and attack to GPU if available\n",
    "        state_batch = torch.FloatTensor(x0).to(device)\n",
    "        next_state_batch = torch.FloatTensor(x1).to(device)\n",
    "        action_batch = torch.LongTensor(a).to(device)\n",
    "        reward_batch = torch.FloatTensor(r).to(device)\n",
    "        done_batch = torch.FloatTensor(1. - d).to(device)\n",
    "\n",
    "        # get train net Q values\n",
    "        train_q = self.train_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # get target net Q values\n",
    "        with torch.no_grad():\n",
    "            # Vanilla DQN: get target values from target net\n",
    "#             target_net_q = reward_batch + done_batch * discount * \\\n",
    "#                      torch.max( self.target_net(next_state_batch).detach(), dim=1)[0].view(batch_size, -1)\n",
    "\n",
    "            # Double DQN: get argmax values from train network, use argmax in target network\n",
    "            train_argmax = self.train_net(next_state_batch).max(1)[1].view(batch_size, 1)\n",
    "            target_net_q = reward_batch + done_batch * discount * \\\n",
    "                    self.target_net(next_state_batch).gather(1, train_argmax)\n",
    "            \n",
    "        # get loss between train q values and target q values\n",
    "            # DQN implementations typically use MSE loss or Huber loss (smooth_l1_loss is similar to Huber)\n",
    "        #loss_fn = nn.MSELoss()\n",
    "        #loss = loss_fn(train_q, target_net_q) \n",
    "        loss = F.smooth_l1_loss(train_q, target_net_q)\n",
    "        \n",
    "        # optimize the parameters with the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.train_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        # we return the loss so we can monitor loss and debug the network if necessary\n",
    "        return loss.detach().cpu().numpy()   \n",
    "    \n",
    "    \n",
    "    def update_target_network(self, num_iter, update_every):\n",
    "        # update target network every so often\n",
    "        # hard target network update: updates target network fully with train network params\n",
    "        if num_iter % update_every == 0:\n",
    "            #print(\"Updating target network parameters\")\n",
    "            self.target_net.load_state_dict(self.train_net.state_dict())    \n",
    "        \n",
    "\n",
    "    def update_target_network_soft(self, num_iter, update_every, update_tau=0.001):\n",
    "        # soft target network update: update target network with mixture of train and target\n",
    "        if num_iter % update_every == 0:\n",
    "            for target_var, var in zip(self.target_net.parameters(), self.train_net.parameters()):\n",
    "                target_var.data.copy_((1.-update_tau) * target_var.data + (update_tau) * var.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Network Updating\n",
    "\n",
    "DQN has two networks: a train network and a target network. The target network provides the next-state Q values for the Q learning update. We use the next-state Q values to update the train network. Periodically we then update target network variables with the train network variables. Using a target network increases stability and convergence of the DQN.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Hyperparameters are a variety of arguments that are usually tuned to help produce the best performing agents. Hyperparameter choice can be tricky as poor hyperparameter choice can prevent an agent from learning anything. The best choice depends on the algorithm and problem. Two basic methods that are used are random search, where you randomly pick hyperparameter values from a predefined range of values, and grid search where you systematically try all values and combinations based on a predefined grid of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize env and set up hyperparameters\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "# wrap env\n",
    "#env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# set seed\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# hyperparameters\n",
    "timesteps = 2000000 # run env for this many time steps\n",
    "hidden_size = 512   # side of hidden layer of FFNN that connects CNN to outputs\n",
    "learning_rate = 0.00001 # learning rate of optimizer\n",
    "batch_size = 64    # size of batch trained on\n",
    "start_training_after = 10001 # start training NN after this many timesteps\n",
    "discount = 0.99 # discount future states by\n",
    "\n",
    "epsilon_start = 1.0 # epsilon greedy start value\n",
    "epsilon_min = 0.01  # epsilon greedy end value\n",
    "epsilon_decay_steps = timesteps * .2 # decay epsilon over this many timesteps\n",
    "epsilon_step = (epsilon_start - epsilon_min)/(epsilon_decay_steps) # decrement epsilon by this amount every timestep\n",
    "\n",
    "update_target_every = 1 # update target network every this steps\n",
    "tau = 0.001\n",
    "\n",
    "# create replay buffer\n",
    "replay_size = 100000 # size of replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=replay_size)\n",
    "\n",
    "# create DQN Agent\n",
    "dqn_agent = DQNAgent(action_size, hidden_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RL Training Loop\n",
    "\n",
    "In the loop, we iterate through our episodes and then the timesteps within the episodes. We stop training the agent based on some stopping condition like a max episode value, a max timestep value, or if the agent's performance reaches a certain level.\n",
    "\n",
    "In each episode and at each timestep the agent selects an action, calls env.step(), and stores the sample in the replay buffer. The agent will also train the training neural network and periodically update the target network. When an episode is done the env is reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619 0.9984679750000333\n",
      "torch.Size([1, 210, 160, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 4, 8, 8], expected input[1, 210, 160, 3] to have 4 channels, but got 210 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1848037/1888736839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# select an action from the agent's policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# decay epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1848037/3317100384.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, s, eps, ts)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0minput_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.view(1, 1)#.detach().cpu().numpy()[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1848037/1656849379.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 4, 8, 8], expected input[1, 210, 160, 3] to have 4 channels, but got 210 channels instead"
     ]
    }
   ],
   "source": [
    "stats_rewards_list = [] # store stats for plotting in this\n",
    "stats_every = 10 # print stats every this many episodes\n",
    "total_reward = 0\n",
    "episode = 1\n",
    "episode_length = 0\n",
    "stats_loss = 0.\n",
    "epsilon = epsilon_start\n",
    "state = env.reset()\n",
    "\n",
    "for ts in range(timesteps):\n",
    "    #env.render()\n",
    "    # select an action from the agent's policy\n",
    "    action = dqn_agent.select_action(state, epsilon, ts)\n",
    "    # decay epsilon\n",
    "    epsilon -= epsilon_step\n",
    "    if epsilon < epsilon_min:\n",
    "        epsilon = epsilon_min\n",
    "            \n",
    "    # enter action into the env\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    episode_length += 1\n",
    "    \n",
    "    # add experience to replay buffer\n",
    "    replay_buffer.add((state, next_state, action, reward, float(done)))\n",
    "    \n",
    "    if ts > start_training_after:\n",
    "        # train the agent\n",
    "        stats_loss += dqn_agent.train(replay_buffer, batch_size, discount)\n",
    "        # update the target network every (if conditions are met in update_target_network)\n",
    "        #dqn_agent.update_target_network(ts, update_target_every) # hard target update\n",
    "        dqn_agent.update_target_network_soft(ts, update_target_every, tau) # soft target update\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        stats_rewards_list.append((episode, total_reward, episode_length))\n",
    "        episode += 1\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        if ts > start_training_after and episode % stats_every == 0:\n",
    "            print('Episode: {}'.format(episode),\n",
    "                'Timestep: {}'.format(ts),\n",
    "                'Total reward: {:.1f}'.format(np.mean(stats_rewards_list[-stats_every:],axis=0)[1]),\n",
    "                'Episode length: {:.1f}'.format(np.mean(stats_rewards_list[-stats_every:],axis=0)[2]),\n",
    "                'Epsilon: {:.2f}'.format(epsilon),\n",
    "                'Loss: {:.4f}'.format(stats_loss))\n",
    "            stats_loss = 0.\n",
    "        \n",
    "        # stopping condition for training if agent reaches the amount of reward\n",
    "        if len(stats_rewards_list) > stats_every and np.mean(stats_rewards_list[-stats_every:],axis=0)[1] > 19:\n",
    "            print(\"Stopping at episode {} with average rewards of {} in last {} episodes\".\n",
    "                format(episode, np.mean(stats_rewards_list[-stats_every:],axis=0)[1], stats_every))\n",
    "            break  \n",
    "    else:\n",
    "        state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot stats\n",
    "def get_running_stat(stat, stat_len):\n",
    "    cum_sum = np.cumsum(np.insert(stat, 0, 0)) \n",
    "    return (cum_sum[stat_len:] - cum_sum[:-stat_len]) / stat_len\n",
    "\n",
    "episode, r, l = np.array(stats_rewards_list).T\n",
    "cum_r = get_running_stat(r, 10)\n",
    "cum_l = get_running_stat(l, 10)\n",
    "\n",
    "# plot rewards\n",
    "plt.plot(episode[-len(cum_r):], cum_r)\n",
    "plt.plot(episode, r, alpha=0.5)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot episode lengths\n",
    "plt.plot(episode[-len(cum_l):], cum_l)\n",
    "plt.plot(episode, l, alpha=0.5)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
